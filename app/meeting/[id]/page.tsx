"use client";

import { useCallback, useEffect, useState } from "react";
import { useParams } from "next/navigation";
import { createClient } from "@supabase/supabase-js";
import { Button } from "@/components/ui/button";
import OpenAI from "openai";

const supabase = createClient(
  process.env.NEXT_PUBLIC_SUPABASE_URL!,
  process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY!
);

const openai = new OpenAI({
  apiKey: process.env.NEXT_PUBLIC_OPENAI_API_KEY,
  dangerouslyAllowBrowser: true,
});

const systemPrompt = `
You are an AI assistant specializing in transcription post-processing 
and language correction. Your task is to improve the accuracy, readability,
and coherence of transcriptions generated by Whisper AI. Ensure the final output is well-structured,
grammatically correct, and preserves the original meaning of the spoken words.
`;

export default function MeetingDetail() {
  const { id } = useParams();
  const [meeting, setMeeting] = useState<{
    name: string;
    transcription: string;
    audio_url: string;
    summary: string;
  } | null>(null);
  const [loading, setLoading] = useState(true);
  const [summarizing, setSummarizing] = useState(false);
  const [transcribing, setTranscribing] = useState(false);

  const fetchMeeting = useCallback(async () => {
    const { data, error } = await supabase
      .from("meetings")
      .select("*")
      .eq("id", id)
      .single();
    if (!error) {
      setMeeting(data);
    }
    setLoading(false);
  }, [id]);

  useEffect(() => {
    if (id) {
      fetchMeeting();
    }
  }, [fetchMeeting, id]);

  const summarizeMeeting = async () => {
    if (!meeting?.transcription) return;
    setSummarizing(true);

    const systemPrompt = `
    You are an AI assistant specializing in summarizing meetings. 
    Summarize the following transcript concisely, only based on the provided text. 
    If the text contains only greetings or does not provide meaningful details, return a minimal summary
    based strictly on the input.
    `;

    try {
      const completion = await openai.chat.completions.create({
        model: "gpt-4o",
        messages: [
          {
            role: "system",
            content: systemPrompt,
          },
          {
            role: "user",
            content: meeting.transcription,
          },
        ],
      });
      const generatedSummary = completion.choices[0].message.content || "";

      setMeeting((prev) => {
        if (!prev) return null;
        return {
          ...prev,
          summary: generatedSummary,
        };
      });
      saveSummaryToSupabase(generatedSummary ?? "");
    } catch (error) {
      console.error("Error summarizing:", error);
      setMeeting((prev) => {
        if (!prev) return null;
        return {
          ...prev,
          summary: "Failed to generate summary.",
        };
      });
    }
    setSummarizing(false);
  };

  const saveSummaryToSupabase = async (summaryText: string) => {
    const { error } = await supabase
      .from("meetings")
      .update({ summary: summaryText })
      .eq("id", id);

    if (error) {
      console.error("Error saving summary to Supabase:", error);
    }
  };

  const transcribeAudio = async () => {
    if (!meeting?.audio_url) return;
    setTranscribing(true);

    const audio_file = await fetch(meeting?.audio_url);

    try {
      const transcription = await openai.audio.transcriptions.create({
        file: audio_file,
        model: "whisper-1",
      });

      const result = transcription.text;
      const correctedText = await processWithGPT4O(result);
      setMeeting((prev) => {
        if (!prev) return null;
        return {
          ...prev,
          transcription: correctedText ?? "",
        };
      });
      await updateTranscriptionInDatabase(correctedText ?? "");
    } catch (error) {
      console.error("Error transcribing audio:", error);
    } finally {
      setTranscribing(false);
    }
  };

  const processWithGPT4O = async (transcript: string) => {
    const completion = await openai.chat.completions.create({
      model: "gpt-4o",
      messages: [
        {
          role: "system",
          content: systemPrompt,
        },
        {
          role: "user",
          content: transcript,
        },
      ],
    });
    return completion.choices[0].message.content;
  };

  const updateTranscriptionInDatabase = async (text: string) => {
    const { error } = await supabase
      .from("meetings")
      .update({ transcription: text })
      .eq("id", id);

    if (error) {
      console.error("Error updating transcription in database:", error);
    }
  };

  if (loading) {
    return <p className="text-center p-6">Loading...</p>;
  }

  if (!meeting) {
    return <p className="text-center p-6">Meeting not found.</p>;
  }

  return (
    <div className="p-6 max-w-2xl mx-auto">
      <h1 className="text-2xl font-bold">{meeting?.name}</h1>
      <audio controls>
        <source src={meeting?.audio_url} type="audio/wav" />
      </audio>
      {meeting?.audio_url && (
        <div className="mt-4 flex gap-2">
          <Button onClick={transcribeAudio}>
            {transcribing ? "Transcribing..." : "Transcribe"}
          </Button>
        </div>
      )}
      <div className="mt-4">
        <h2 className="text-lg font-semibold">Transcription</h2>
        <p className="bg-gray-100 p-4 rounded mt-2">{meeting?.transcription}</p>
      </div>
      {meeting?.transcription && (
        <div className="mt-4">
          <Button onClick={summarizeMeeting} disabled={summarizing}>
            {summarizing ? "Summarizing..." : "Summarize"}
          </Button>
          {meeting?.summary && (
            <div className="mt-4 p-4 bg-blue-100 rounded">
              <h2 className="text-lg font-semibold">Summary</h2>
              <p>{meeting?.summary}</p>
            </div>
          )}
        </div>
      )}
    </div>
  );
}
